{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['F5.wav', 'F4.wav', 'F3.wav', 'F2.wav', 'F1.wav', 'M1.wav', 'M2.wav', 'M3.wav', 'M4.wav', 'M5.wav']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "from types import SimpleNamespace\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from scipy.io import wavfile\n",
    "from scipy.signal import correlate\n",
    "from scipy import signal as sg\n",
    "import time\n",
    "import scipy\n",
    "from matplotlib import pyplot as plt\n",
    "import librosa\n",
    "import sklearn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"data/pitch/test\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "0c7be69d4071f45f46430daf4015fa8c1145d4da"
   },
   "outputs": [],
   "source": [
    "class Counters:\n",
    "    def __init__(self, gross_threshold=0.2):\n",
    "        self.num_voiced = 0\n",
    "        self.num_unvoiced = 0\n",
    "        self.num_voiced_unvoiced = 0\n",
    "        self.num_unvoiced_voiced = 0\n",
    "        self.num_voiced_voiced = 0\n",
    "        self.num_gross_errors = 0\n",
    "        self.fine_error = 0\n",
    "        self.e2 = 0\n",
    "        self.gross_threshold = gross_threshold\n",
    "        self.nfiles = 0\n",
    "\n",
    "    def add(self, other):\n",
    "        if other is not None:\n",
    "            self.num_voiced += other.num_voiced\n",
    "            self.num_unvoiced += other.num_unvoiced\n",
    "            self.num_voiced_unvoiced += other.num_voiced_unvoiced\n",
    "            self.num_unvoiced_voiced += other.num_unvoiced_voiced\n",
    "            self.num_voiced_voiced += other.num_voiced_voiced\n",
    "            self.num_gross_errors += other.num_gross_errors\n",
    "            self.fine_error += other.fine_error\n",
    "            self.e2 += other.e2\n",
    "            self.nfiles += 1\n",
    "\n",
    "    def __repr__(self):\n",
    "        nframes = self.num_voiced + self.num_unvoiced\n",
    "        if self.nfiles > 0:\n",
    "            self.fine_error /= self.nfiles\n",
    "        str = [\n",
    "            f\"Num. frames:\\t{self.num_unvoiced + self.num_voiced} = {self.num_unvoiced} unvoiced + {self.num_voiced} voiced\",\n",
    "            f\"Unvoiced frames as voiced:\\t{self.num_unvoiced_voiced}/{self.num_unvoiced} ({100*self.num_unvoiced_voiced/self.num_unvoiced:.2f}%)\",\n",
    "            f\"Voiced frames as unvoiced:\\t{self.num_voiced_unvoiced}/{self.num_voiced} ({100*self.num_voiced_unvoiced/self.num_voiced:.2f}%)\",\n",
    "            f\"Gross voiced errors (>{100*self.gross_threshold}%):\\t{self.num_gross_errors}/{self.num_voiced_voiced} ({100*self.num_gross_errors/self.num_voiced_voiced:.2f}%)\",\n",
    "            f\"MSE of fine errors:\\t{100*self.fine_error:.2f}%\",\n",
    "            f\"RMSE:\\t{np.sqrt(self.e2/nframes):.2f}\"\n",
    "        ]\n",
    "        return '\\n'.join(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "e9870d63a5eb733ef6b98f3305b3c2634608bf3f"
   },
   "outputs": [],
   "source": [
    "def compare(fref, pitch):\n",
    "    vref = np.loadtxt(fref)\n",
    "    vtest = np.array(pitch)\n",
    "\n",
    "    diff_frames = len(vref) - len(vtest)\n",
    "    if abs(diff_frames) > 5:\n",
    "        print(f\"Error: number of frames in ref ({len(vref)}) != number of frames in test ({len(vtest)})\")\n",
    "        return None\n",
    "    elif diff_frames > 0:\n",
    "        vref = np.resize(vref, vtest.shape)\n",
    "    elif diff_frames < 0:\n",
    "        vtest = np.resize(vtest, vref.shape)\n",
    "\n",
    "    counters = Counters()\n",
    "    counters.num_voiced = np.count_nonzero(vref)\n",
    "    counters.num_unvoiced = len(vref) - counters.num_voiced\n",
    "    counters.num_unvoiced_voiced = np.count_nonzero(np.logical_and(vref == 0, vtest != 0))\n",
    "    counters.num_voiced_unvoiced = np.count_nonzero(np.logical_and(vref != 0, vtest == 0))\n",
    "\n",
    "    voiced_voiced = np.logical_and(vref != 0, vtest != 0)\n",
    "    counters.num_voiced_voiced = np.count_nonzero(voiced_voiced)\n",
    "\n",
    "    f = np.absolute(vref[voiced_voiced] - vtest[voiced_voiced])/vref[voiced_voiced]\n",
    "    gross_errors = f > counters.gross_threshold\n",
    "    counters.num_gross_errors = np.count_nonzero(gross_errors)\n",
    "    fine_errors = np.logical_not(gross_errors)\n",
    "    counters.fine_error = np.sqrt(np.square(f[fine_errors]).mean())\n",
    "    counters.e2 = np.square(vref - vtest).sum()\n",
    "\n",
    "    return counters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrelation_method(frame, sfreq, threshold=0.55, fmin=40, fmax=400, verbose=False):\n",
    "    \"\"\"Estimate pitch using autocorrelation\n",
    "    \"\"\"\n",
    "    # Preprocessing options\n",
    "    windowing = False\n",
    "    smoothing_gaussian = False\n",
    "    smoothing_savgol_filter = True\n",
    "    \n",
    "    if verbose:\n",
    "        plt.figure()\n",
    "        plt.title('Frame')\n",
    "        plt.plot(frame)\n",
    "        plt.xlabel('Samples')\n",
    "        plt.show\n",
    "        \n",
    "    if windowing:\n",
    "        window = sg.windows.hann(len(frame))\n",
    "        frame = frame*window\n",
    "        if verbose:\n",
    "            plt.figure()\n",
    "            plt.title('Windowed Frame')\n",
    "            plt.plot(frame)\n",
    "            plt.xlabel('Samples')\n",
    "            plt.show\n",
    "        \n",
    "    if smoothing_gaussian:\n",
    "        std = 0.5\n",
    "        frame = scipy.ndimage.gaussian_filter1d(frame, std)\n",
    "        if verbose:\n",
    "            plt.figure()\n",
    "            plt.title('Gaussian Smoothed Frame')\n",
    "            plt.plot(frame)\n",
    "            plt.xlabel('Samples')\n",
    "            plt.show\n",
    "    \n",
    "    if smoothing_savgol_filter:\n",
    "        window_lenght = 15\n",
    "        order = 2\n",
    "        frame = sg.savgol_filter(frame, window_lenght, order)\n",
    "        if verbose:\n",
    "            plt.figure()\n",
    "            plt.title('Savgol Filter Smoothed Frame')\n",
    "            plt.plot(frame)\n",
    "            plt.xlabel('Samples')\n",
    "            plt.show\n",
    "\n",
    "    # Calculate autocorrelation using scipy correlate\n",
    "    frame = frame.astype(np.float)\n",
    "    frame -= frame.mean()\n",
    "    amax = np.abs(frame).max()\n",
    "    if amax > 0:\n",
    "        frame /= amax\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    corr = correlate(frame, frame)\n",
    "    # keep the positive part\n",
    "    corr = corr[len(corr)//2:]\n",
    "\n",
    "    # Find the first minimum\n",
    "    dcorr = np.diff(corr)\n",
    "    rmin = np.where(dcorr > 0)[0]\n",
    "    if len(rmin) > 0:\n",
    "        rmin1 = rmin[0]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "    # Find the next peak\n",
    "    peak = np.argmax(corr[rmin1:]) + rmin1\n",
    "    rmax = corr[peak]/corr[0]\n",
    "    f0 = sfreq / peak\n",
    "\n",
    "    if rmax > threshold and f0 >= fmin and f0 <= fmax:\n",
    "        return f0\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM-based Voice Activity Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extractor function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vad_features(audio_file, options): # data is the loaded audio file, fs = sampling frequency\n",
    "    data, sfreq = librosa.load(audio_file)\n",
    "    \n",
    "    # From miliseconds to samples\n",
    "    ns_windowlength = int(round((options.windowlength * sfreq) / 1000))\n",
    "    ns_frameshift = int(round((options.frameshift * sfreq) / 1000))\n",
    "    ns_left_padding = int(round((options.left_padding * sfreq) / 1000))\n",
    "    ns_right_padding = int(round((options.right_padding * sfreq) / 1000))\n",
    "    nsamples = len(data)\n",
    "\n",
    "    zcr = librosa.feature.zero_crossing_rate(data, frame_length=ns_windowlength, hop_length=ns_frameshift)\n",
    "    rmse = librosa.feature.rmse(y=data, frame_length=ns_windowlength, hop_length=ns_frameshift)\n",
    "    power = []\n",
    "    for id, ini in enumerate(range(-ns_left_padding, nsamples - ns_windowlength + ns_right_padding + 1, ns_frameshift)):\n",
    "        first_sample = max(0, ini)\n",
    "        last_sample = min(nsamples, ini + ns_windowlength)\n",
    "        frame = data[first_sample:last_sample]\n",
    "        power.append(10*np.log10(np.mean([sample**2 for sample in frame])))\n",
    "\n",
    "    power_np = np.array(power)\n",
    "    features = np.vstack((zcr, rmse, power_np))\n",
    "    features = features.T\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm_vad(options, gui):\n",
    "    fs = open(options.submission, 'w') if options.submission is not None else None\n",
    "    totalCounters = Counters()\n",
    "    print('Extracting features...')\n",
    "    with open(gui) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            filename = os.path.join(options.datadir, line + \".wav\")\n",
    "            f0ref_filename = os.path.join(options.datadir, line + \".f0ref\")\n",
    "            \n",
    "            data, sfreq = librosa.load(filename)\n",
    "            freqs_ref = np.loadtxt(f0ref_filename)\n",
    "            \n",
    "            labels = []\n",
    "            for a in freqs_ref:\n",
    "                if int(a)==0:\n",
    "                    labels.append(0)\n",
    "                elif int(a)>0:\n",
    "                    labels.append(1)\n",
    "\n",
    "            # From miliseconds to samples\n",
    "            ns_windowlength = int(round((options.windowlength * sfreq) / 1000))\n",
    "            ns_frameshift = int(round((options.frameshift * sfreq) / 1000))\n",
    "            ns_left_padding = int(round((options.left_padding * sfreq) / 1000))\n",
    "            ns_right_padding = int(round((options.right_padding * sfreq) / 1000))\n",
    "            nsamples = len(data)\n",
    "\n",
    "            zcr = librosa.feature.zero_crossing_rate(data, frame_length=ns_windowlength, hop_length=ns_frameshift)\n",
    "            rmse = librosa.feature.rmse(y=data, frame_length=ns_windowlength, hop_length=ns_frameshift)\n",
    "            power = []\n",
    "            # mfccs = []\n",
    "            for id, ini in enumerate(range(-ns_left_padding, nsamples - ns_windowlength + ns_right_padding + 1, ns_frameshift)):\n",
    "                first_sample = max(0, ini)\n",
    "                last_sample = min(nsamples, ini + ns_windowlength)\n",
    "                frame = data[first_sample:last_sample]\n",
    "                power.append(10*np.log10(np.mean([sample**2 for sample in frame])))\n",
    "                # mfccs.append(np.array(librosa.feature.mfcc(y=frame, sr=fs)))\n",
    "                \n",
    "    power_np = np.array(power)\n",
    "    features = np.vstack((zcr, rmse, power_np))\n",
    "    features = features.T\n",
    "\n",
    "    model = sklearn.svm.SVC(kernel='linear')\n",
    "    print('model:')\n",
    "    print(model)\n",
    "    print('Training...')\n",
    "    model.fit(features, labels)\n",
    "    print('Train evaluation...')\n",
    "    model.score(features, labels)\n",
    "    print('Mean Accuracy: ', model.score(features, labels))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features...\n",
      "model:\n",
      "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
      "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
      "  shrinking=True, tol=0.001, verbose=False)\n",
      "Training...\n",
      "Train evaluation...\n",
      "Mean Accuracy:  0.9138576779026217\n",
      "Required time: 61.43123197555542\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "fda_ue_options = SimpleNamespace(\n",
    "    windowlength=32, frameshift=15, left_padding=16, right_padding=16, datadir='data', submission=None)\n",
    "model_svm = train_svm_vad(fda_ue_options, 'data/pitch/fda_ue.gui')\n",
    "print(f'Required time: {time.time() - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP-based VAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp_vad(options, gui):\n",
    "    fs = open(options.submission, 'w') if options.submission is not None else None\n",
    "    totalCounters = Counters()\n",
    "    print('Extracting features...')\n",
    "    with open(gui) as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            filename = os.path.join(options.datadir, line + \".wav\")\n",
    "            f0ref_filename = os.path.join(options.datadir, line + \".f0ref\")\n",
    "            \n",
    "            data, sfreq = librosa.load(filename)\n",
    "            freqs_ref = np.loadtxt(f0ref_filename)\n",
    "            \n",
    "            labels = []\n",
    "            for a in freqs_ref:\n",
    "                if int(a)==0:\n",
    "                    labels.append(0)\n",
    "                elif int(a)>0:\n",
    "                    labels.append(1)\n",
    "\n",
    "            # From miliseconds to samples\n",
    "            ns_windowlength = int(round((options.windowlength * sfreq) / 1000))\n",
    "            ns_frameshift = int(round((options.frameshift * sfreq) / 1000))\n",
    "            ns_left_padding = int(round((options.left_padding * sfreq) / 1000))\n",
    "            ns_right_padding = int(round((options.right_padding * sfreq) / 1000))\n",
    "            nsamples = len(data)\n",
    "\n",
    "            zcr = librosa.feature.zero_crossing_rate(data, frame_length=ns_windowlength, hop_length=ns_frameshift)\n",
    "            rmse = librosa.feature.rmse(y=data, frame_length=ns_windowlength, hop_length=ns_frameshift)\n",
    "            power = []\n",
    "            # mfccs = []\n",
    "            for id, ini in enumerate(range(-ns_left_padding, nsamples - ns_windowlength + ns_right_padding + 1, ns_frameshift)):\n",
    "                first_sample = max(0, ini)\n",
    "                last_sample = min(nsamples, ini + ns_windowlength)\n",
    "                frame = data[first_sample:last_sample]\n",
    "                power.append(10*np.log10(np.mean([sample**2 for sample in frame])))\n",
    "                # mfccs.append(np.array(librosa.feature.mfcc(y=frame, sr=fs)))\n",
    "                \n",
    "    power_np = np.array(power)\n",
    "    features = np.vstack((zcr, rmse, power_np))\n",
    "    features = features.T\n",
    "\n",
    "    model = MLPClassifier()\n",
    "    print('model:')\n",
    "    print(model)\n",
    "    print('Training...')\n",
    "    model.fit(features, labels)\n",
    "    print('Train evaluation...')\n",
    "    model.score(features, labels)\n",
    "    print('Mean Accuracy: ', model.score(features, labels))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features...\n",
      "model:\n",
      "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(100,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n",
      "Training...\n",
      "Train evaluation...\n",
      "Mean Accuracy:  0.9138576779026217\n",
      "Required time: 67.46067690849304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oscarortigonzalez/Projects/pitch-estimation-notebooks/.venv/lib/python3.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "fda_ue_options = SimpleNamespace(\n",
    "    windowlength=32, frameshift=15, left_padding=16, right_padding=16, datadir='data', submission=None)\n",
    "model_mlp = train_mlp_vad(fda_ue_options, 'data/pitch/fda_ue.gui')\n",
    "print(f'Required time: {time.time() - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Validation and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "37f3e70917faeb9e5f47d1d53b910ce07bc944a3"
   },
   "outputs": [],
   "source": [
    "def wav2f0(options, gui):\n",
    "    fs = open(options.submission, 'w') if options.submission is not None else None\n",
    "    totalCounters = Counters()\n",
    "    with open(gui) as f:\n",
    "        if fs is not None:\n",
    "            print('id,frequency', file=fs)\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            filename = os.path.join(options.datadir, line + \".wav\")\n",
    "            f0ref_filename = os.path.join(options.datadir, line + \".f0ref\")\n",
    "            print(\"Processing:\", filename)\n",
    "            sfreq, data = wavfile.read(filename)\n",
    "            nsamples = len(data)\n",
    "            \n",
    "            # Get VAD features\n",
    "            vad_features = extract_vad_features(filename, options)\n",
    "            vad_results = model_svm.predict(vad_features)\n",
    "            \n",
    "            # From miliseconds to samples\n",
    "            ns_windowlength = int(round((options.windowlength * sfreq) / 1000))\n",
    "            ns_frameshift = int(round((options.frameshift * sfreq) / 1000))\n",
    "            ns_left_padding = int(round((options.left_padding * sfreq) / 1000))\n",
    "            ns_right_padding = int(round((options.right_padding * sfreq) / 1000))\n",
    "            pitch = []\n",
    "            i = 0\n",
    "            for id, ini in enumerate(range(-ns_left_padding, nsamples - ns_windowlength + ns_right_padding + 1, ns_frameshift)):\n",
    "                # Check if it is a voiced frame\n",
    "                if vad_results[i] == 1: \n",
    "                    first_sample = max(0, ini)\n",
    "                    last_sample = min(nsamples, ini + ns_windowlength)\n",
    "                    frame = data[first_sample:last_sample]\n",
    "                    f0 = autocorrelation_method(frame, sfreq, verbose=False)\n",
    "                else:\n",
    "                    f0 = 0\n",
    "                if fs is not None:\n",
    "                    print(line + '_' + str(id) + ',', f0, file=fs)\n",
    "                pitch.append(f0)\n",
    "                i += 1\n",
    "                \n",
    "            # Apply median filter to pitch array\n",
    "            pitch = sg.medfilt(volume=pitch, kernel_size=3)\n",
    "\n",
    "            if os.path.isfile(f0ref_filename):\n",
    "                counters = compare(f0ref_filename, pitch)\n",
    "                totalCounters.add(counters)\n",
    "\n",
    "\n",
    "    if totalCounters.num_voiced + totalCounters.num_unvoiced > 0:\n",
    "        print(f\"### Summary\")\n",
    "        print(totalCounters)\n",
    "        print(\"-------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: data/fda_ue/rl001.wav\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 100 is out of bounds for axis 0 with size 100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-de240d94f828>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m fda_ue_options = SimpleNamespace(\n\u001b[1;32m      3\u001b[0m     windowlength=32, frameshift=15, left_padding=16, right_padding=16, datadir='data', submission=None)\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mwav2f0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfda_ue_options\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data/pitch/fda_ue.gui'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Required time: {time.time() - start_time}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-f42105fac46f>\u001b[0m in \u001b[0;36mwav2f0\u001b[0;34m(options, gui)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mini\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mns_left_padding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnsamples\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mns_windowlength\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mns_right_padding\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns_frameshift\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0;31m# Check if it is a voiced frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mvad_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                     \u001b[0mfirst_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mini\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0mlast_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnsamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mini\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mns_windowlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 100 is out of bounds for axis 0 with size 100"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "fda_ue_options = SimpleNamespace(\n",
    "    windowlength=32, frameshift=15, left_padding=16, right_padding=16, datadir='data', submission=None)\n",
    "wav2f0(fda_ue_options, 'data/pitch/fda_ue.gui')\n",
    "print(f'Required time: {time.time() - start_time}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "74599d24c27013ae1a90007812b76e9dc576a903"
   },
   "outputs": [],
   "source": [
    "test_options = SimpleNamespace(\n",
    "    windowlength=26.5, frameshift=10, left_padding=13.25, right_padding=7, datadir='data/pitch/test', submission='autocorrelation_method_with_SVM_VAD_submission.csv')\n",
    "wav2f0(test_options, 'data/pitch/test.gui')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
